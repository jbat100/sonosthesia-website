
.container

    p.quotation (Under construction...)

    h1 Interactive Digital Arts

    p The idea of interactive digital art is not new. It essentially emerged as soon as computer technology allowed it to, continuing a tradition of audio/visual art which preceded it in spirit but was neither digital nor interactive. It was elegantly formulated by Golan Levin on the #[a(href="http://acg.media.mit.edu/people/golan/aves/") Audiovisual Environment Suite] website, a project which goes back to the year 2000.

    div.container
        p.quotation "The Audiovisual Environment Suite [...] allows people to create and perform abstract animation and synthetic sound in real time. [It] attempts to design an interface which is supple and easy to learn, yet can also yield interesting, infinitely variable and personally expressive performances in both the visual and aural domains. Ideally, these systems permit their interactants to engage in a flow state of pure experience. [It is] built around the metaphor of an inexhaustible and dynamic audiovisual substance, which is freely deposited and controlled by the user's gestures. Each instrument situates this substance in a context whose free-form structure inherits from the visual language of abstract painting and animation. The use of low-level synthesis techniques permits the sound and image to be tightly linked, commensurately malleable, and deeply plastic."

    p An extended discussion of the audio/visual arts tradition as well as considerations for a new interface metaphor for digital media, and descriptions of substantial list of important works in the domain can be read in Golan Levin's #[a(href="http://www.flong.com/texts/publications/thesis/") Masters Thesis]. This vision is central to Sonosthesia. We aim to create inter-modal control flows where content generated in different forms of media, by different software components, affect each other in real-time, as well as being influenced by user input in powerfully expressive ways. As computer technology and interaction techniques evolve the possibilities for the real-time creation of complex audio/visual content grow with it, and although a number of projects have since continued Golan's work, opportunities provided by a new generation of tools seem to be yet untapped.


    h1 Standards for Domain Specific Control Data

    p A central concept of Sonosthesia cross-modal control flow between components. Components can react to control input and produce control output. Digital art often uses open-ended protocols such as OSC, which are very flexible but lack the kind of standardised semantics enforced by older protocols such as MIDI. Creating a reference for domain specific control data to allow compatibility between different components is an important step, depending on the type of component this control data will take different forms. JSON is used for data representation as it combines flexibility with an explicit and human readable format, it is also transport independent.

    h2 Sound Synthesis Control Data

    p When it comes to sound and music, the persistence of MIDI for sound control despite its limitations (7 bit depth for channel, pitch, small number of channels, limited control and after-touch etc...) shows the power of standardization. The first step in a sound synthesis control standard should be to emulate MIDI, removing some of the previous limitations. An example sonosthesia control message emulating MIDI #[b note_on]


    :markdown
        ```javascript
            {
                "type": "synth",
                "subtype": "start",
                "channel": "pad1",
                "identifier": "123e4567-e89b-12d3-a456-426655440000",
                "parameters": {
                    "pitch": "A#5",
                    "volume": 0.5433435,
                    "shine": 0.232838
                }
            }
        ```

    p Labelled string fields are human readable, but also give a lot of additional flexibility. A typical example illustrated in #[a(href="https://www.native-instruments.com/forum/threads/using-osc-to-trigger-notes.229614/") this thread], shows the problems which can be encountered when trying to express micro-tonal, non-western musical forms using MIDI. This could easily be addressed by offering different pitch definition options, in hertz

    :markdown
        ```javascript
        {
            "pitch": "f_442"
        }
        ```

    p This additional flexibility can cause problems when applying MIDI control messages (per channel, after touch etc...) because the same routing mechanisms cannot be used. Enter the obscure #[b identifier] field in the original #[b synth::note_on] message. This non-human-readable, machine-generated identifier can be used to reference note instances giving hugely flexible #[b control] and #[b stop] messages. These messages can be addressed to multiple note instances or to whole channels by giving an array as #[b identifier] or replacing the #[b identifier] field by a #[b channel] field. Parameter keys can be nested using the dot syntax to make groups more explicit. Some example control messages:

    :markdown
        ```javascript
        // control
        {
            "type": "synth",
            "subtype": "control",
            "identifier": "123e4567-e89b-12d3-a456-426655440000",
            "parameters": {
                "pitch": "f_432"
                "intensity": 0.342342,
                "shine": 0.54345343
            }
        }

        // control for multiple note instances, with parameter nesting
        {
            "type": "synth",
            "subtype": "control",
            "identifier": ["123e4567-e89b-12d3-a456-426655440000", "42665544-e89b-12d3-a456-123e45670000"],
            "parameters": {
                "scratch.brightness": 0.342342,
                "bow.intensity": 0.54345343
            }
        }
        ```

    p Some example stop messages:

    :markdown
        ```javascript

        // stop for a single note
        {
            "type": "synth",
            "subtype": "stop",
            "identifier": "123e4567-e89b-12d3-a456-426655440000",
            "parameters": {
                "release": 0.342342
            }
        }

        // stop for all notes on a channel
         {
             "type": "synth",
             "subtype": "stop",
             "channel": "pad2",
             "parameters": {
                 "release": 0.342342
             }
         }
        ```

    p This additional flexibility in defining a synthesizers attributes and control mechanism comes at a cost of complexity for the synthesizer itself. But it is a price worth paying. It is up to the synthesizers to document the control parameters which they support, and up to the controller to adapt, but basic rules and message structures are in place.


    h2 Virtual Object Manipulation Control Data

    p Virtual object manipulation is of central importance to the Sonosthesia project, it is a complex emerging subject. There is little pre-existing work in terms of protocols describing them. Currently the proposed protocol assumes rigid body physics but is designed to be extended to soft bodies. Manipulations are described in terms of effector (hands, sticks), targets (any object), and collision descriptions.

    :markdown
        ```javascript
        {
            "type": "manipulation",
            "subtype": "contact",
            "identifier": "123e4567-e89b-12d3-a456-426655440000"
            "effector": {
                "identifier": "finger1",
                "position": [12.3423, 35.4323, 76.3432],
                "normal": [0.45, 0.23, 0.87],
                "velocity": [16.2345, 1.3872, 10.34],
                "color": [0.342, 0.564, 0.545, 0.352],
                "uv": [0.342, 0.564]
            },
            "target": {
                "identifier": "ball2",
                "position": [12.3423, 35.4323, 76.3432],
                "normal": [0.87, 0.23, 0.45],
                "velocity": [16.2345, 1.3872, 10.34],
                "color": [0.342, 0.564, 0.545, 0.352],
                "uv": [0.342, 0.564]
            }
        }
        ```

    p Higher level manipulation descriptors are planned.

    h2 Generic Continuous Control Data

    p Having a multipurpose continuous control message is very useful and in many cases is sufficient.

    h2 Generic Discrete Control Data





    h1 Standards for Domain Mappings

    p and generate control streams and mapping between different domains.

    p Defining how these mappings are made is a crucial aspect of the project. As a simple example, a component receives a control stream from a source and reacts to it by producing musical notes and timbral changes. It can then generate descriptors of the content it has generated and feed them into a new control stream of its own. This stream can be piped to another software component (which generates graphical textures, mesh deformations, physical forces in a simulation, or whatever you can imagine). The next software component in the pipeline receives this control data as input and the can in turn generate content and an associated descriptor output control stream. This control stream can be piped into yet another software component, or several, or looped back to the first component, creating an infinite software control loop. Concrete examples may help.


    p Digital art performance has traditionally suffered from the low-dimensionality of typical control devices (sliders, knobs, keys, sometimes light-beams, touch-tables), these all have only one or two degrees of freedom and fall short of exploiting the huge potential of the human brain, which is revealed for example during virtuosic performances on acoustic instruments. Gestural control has been explored as a human-computer interaction (HCI) vector, addressing the issue of low dimensionality to some extent. However, in current forms gestural interfaces suffer from a lack of context. That is gestures are performed in empty space while we would naturally expect the effect of our gestures to depend on their environmental effects.

    h2 Contacts as Sound Synthesis Controllers

    h2 Visual Attributes as Cues for Sonic Properties

    h2 Sound Analysis Data as Generic Controller

