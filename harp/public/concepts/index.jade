
.container


    h2 Interactive Digital Art


    p The idea of interactive digital art is not new. It essentially emerged as soon as computer technology allowed it to, continuing a tradition of audio/visual art which preceded it in spirit but was neither digital nor interactive. It was elegantly formulated by Golan Levin on the #[a(href="http://acg.media.mit.edu/people/golan/aves/") Audiovisual Environment Suite] website, a project which goes back to the year 2000.

    div.container
        p.quotation "The Audiovisual Environment Suite [...] allows people to create and perform abstract animation and synthetic sound in real time. [It] attempts to design an interface which is supple and easy to learn, yet can also yield interesting, infinitely variable and personally expressive performances in both the visual and aural domains. Ideally, these systems permit their interactants to engage in a flow state of pure experience. [It is] built around the metaphor of an inexhaustible and dynamic audiovisual substance, which is freely deposited and controlled by the user's gestures. Each instrument situates this substance in a context whose free-form structure inherits from the visual language of abstract painting and animation. The use of low-level synthesis techniques permits the sound and image to be tightly linked, commensurately malleable, and deeply plastic."

    p An extended discussion of the audio/visual arts tradition as well as considerations for a new interface metaphor for digital media, and descriptions of substantial list of important works in the domain can be read in Golan Levin's #[a(href="http://www.flong.com/texts/publications/thesis/") Masters Thesis]. This vision is central to Sonosthesia. We aim to create inter-modal control flows where content generated in different forms of media, by different software components, affect each other in real-time, as well as being influenced by user input in powerfully expressive ways. As computer technology and interaction techniques evolve the possibilities for the real-time creation of complex audio/visual content grow with it, and although a number of projects have since continued Golan's work, opportunities provided by a new generation of tools seem to be yet untapped.

    h2 Mainstream Appeal

    p The mainstream of digital art consumers is clearly in demand for multi-modal content and already familiar with it to some extent. Concert lighting is often partially guided by the sound produced by a musical performance, music players often have a visualiser option which generate graphical eccentricities driven by sound characteristics. More sophisticated audio-visual art has potential mass appeal, but although there have been numerous research projects developing and implementing new ideas, they have struggled to escape the world of academia, research institutions and heavily technical enthusiasts. This is partly a reflection of the tools that are often used in cutting edge audio/visual art, such as Max, Pure Data or languages like Lua. These tools are extremely powerful but difficult to understand and master, and often intimidating for sound and visual artists who are accustomed to different types of software.

    h2 Formalising Concepts

    p The first objective is to gather, describe and develop ideas for audio/visual interaction paradigms and meaningful abstractions for mapping between different domains, modes or software components. Concepts are discussed extensively in a #[a(href="concepts") dedicated section]. The guideline is the idea of cross-modal control flow between components. The term component is deliberately vague, components produce any combination of visual, audio or haptic content and can run within or across processes, machines or networks. Different components then communicate with and stimulate each other by sending and receiving control streams and mapping between different domains. Determining how these mappings are made is a crucial aspect of the project.

    h3 An Open Control Protocol for Endless Extension

    p From these concepts should emerge a specification which allows different control flows to be implemented in practise. The objective is a set of interoperable tools, plugins and libraries which allow existing software to interact. Having a rigorously specified protocol will facilitate this. The currently proposed protocol builds on top of OSC, which is very simple yet highly flexible and versatile.

    h2 VR as a Controller

    p Emerging technology such as virtual and augmented reality have a huge potential for supporting new highly engaging interaction paradigms, by providing a full blown three dimensional contextual environment for human interaction.

    p Gestural control has been explored as a human-computer interaction (HCI) vector, addressing the issue of low dimensionality to some extent. However in the current form gestural interfaces suffer from a lack of context. That is they are performed in empty space while we would naturally expect the effect of our gestures to depend on their environmental effects.

    p Learning to use gestures as control mechanisms dissociated from their effect on the surrounding environment is somewhat unnatural. and the levels of dexterity witnessed in master musicians or artisans are testament to the potential.

    p Digital art performance has traditionally suffered from the low-dimensionality of typical control devices (sliders, knobs, keys, sometimes light-beams, touch-tables), these all have only one or two degrees of freedom and fall short of exploiting the huge potential of the human brain, which is revealed for example during virtuosic performances on acoustic instruments.

    h2 Sound as a Controller


    p As a simple example, a component receives a control stream from a source and reacts to it by producing musical notes and timbral changes. It can then generate descriptors of the content it has generated and feed them into a new control stream of its own. This stream can be piped to another software component (which generates graphical textures, mesh deformations, physical forces in a simulation, or whatever you can imagine).). The next software component in the pipeline receives this control data as input and the can in turn generate content and an associated descriptor output control stream. This control stream can be piped into yet another software component, or several, or looped back to the first component, creating an infinite software control loop. Concrete examples may help.

