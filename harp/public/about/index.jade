

.container



    h2 Next Generation Interactive Art

    p Sonosthesia is a highly inter-disciplinary project, and although its mission statement is relatively clear, specific objectives will be in constant evolution. Currently, two main concepts are explored, although the aim is to broaden horizons as much as possible. A brief overview is given here, ideas for Sonosthesia are described in more detail in the

    h3 Sound as a Controller

    p Extracting sound descriptors  to generate visual content (both in 2 and 3 dimensions). Analyse audio/MIDI streams in real-time and broadcast the results.

    h3 VR as a Controller

    p Using object manipulations in VR to generate sound control data. Emerging interaction techniques such as virtual and augmented reality have a huge potential for creating highly engaging multi-modal art forms.


    h3 A Vision for the Future

    p The final objective is for virtual worlds to become performance spaces in their own right within which performers can interact with purposely designed virtual objects, with all the power of modern physics engines providing organic, lifelike experiences, and the flexibility to configure these objects as effective media controllers. Collaboration, Audience Immersion.


    h2 Approach

    p The first is game engines (such as Unity, Unreal Game Engine, Blender) used for game design, CGI, HCI and virtual reality. They are essentially a combination of a graphics engine, a physics engine, and some additional tools specifically aimed at facilitating creating 2D/3D/VR gaming experiences. One thing game engines are not good at however is real-time musical sound control as most games do not require much beyond playing spatialised pre-recorded sounds, sonic atmosphere generation, perhaps rudimentary sonification.

    p This is where the second class of software tools comes into play, namely digital audio workstations (such as Logic Pro, Ableton Live or Pro Tools). These are used for music composition and production, sound design, and most importantly live musical performances. They are particularly good at controlling sound synthesis, processing audio streams, creating processing pipelines, sequencing notes, handling large sound libraries, loops, complicated audio setups, routing, mixing and a lot more.

    p These tools are very mature and have a very large existing, highly skilled user base and between them provide a great deal of the functionality that we need. What is missing however is a language for them to interact with each other, sending each other information and control messages.





    h2 Concepts

    p Gather, describe and develop ideas for audio/visual content generation paradigms. The project provides a set of  interoperable tools, plugins and libraries which allow existing software to interact, and to provide meaningful abstractions for mapping different domains.









    h3 Control Protocols Based on OSC

    p We aim to provide a common language for software components to communicate. Specifically, develop effective and generic protocols to provide analysis data and control data, allowing different software components to interact with each other in meaningful ways.


    h3 Audio/MIDI Plugins (VST/AU) as DAW extensions

    p Provide extensions to digital audio workstations (such as Logic Pro, Ableton Live or Pro Tools, through the use of VST or AU plugins and custom built processors and synthesizers), to provide real-time sound descriptors, gather automation and control (MIDI/OSC) data, and broadcast this information on the local network along with control information describing intended effects.

    h3 Scripts as VR engine extensions

    p Extend existing graphics/physics/VR engines (such as Unity, Unreal Game Engine, Blender) using their extensive scripting APIs.


